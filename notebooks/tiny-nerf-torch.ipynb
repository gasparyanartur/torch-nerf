{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env CUDA_LAUNCH_BLOCKING=1\n",
    "%env TORCH_USE_CUDA_DSA=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib as pl\n",
    "import requests\n",
    "\n",
    "def download_file(dst_path: pl.Path, url: str):\n",
    "    res = requests.get(url)\n",
    "    if not res.status_code == 200:\n",
    "        raise ValueError(f\"Failed to download data - Status code:\", res.status_code)\n",
    "\n",
    "    content = res.content\n",
    "    with open(dst_path, \"wb\") as f:\n",
    "        f.write(content)\n",
    "    \n",
    "\n",
    "base_data_path = pl.Path('./data/')\n",
    "tiny_data_url = \"http://cseweb.ucsd.edu/~viscomp/projects/LF/papers/ECCV20/nerf/tiny_nerf_data.npz\"\n",
    "tiny_data_name = \"tiny_nerf_data.npz\"\n",
    "\n",
    "tiny_data_path = base_data_path / tiny_data_name\n",
    "if not tiny_data_path.exists():\n",
    "    download_file(tiny_data_path, tiny_data_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import re\n",
    "import torch\n",
    "import json\n",
    "from torchvision import io\n",
    "from torchvision.transforms import v2 as transforms\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "rgb_digit_pattern = re.compile(\"r_(?P<digit>\\d+).png\")\n",
    "\n",
    "\n",
    "def load_img_paths(imgs_path: pl.Path) -> list[pl.Path]:\n",
    "    # Ordered lexagraphically, rearrange to numberical ordering\n",
    "    digit_path_pairs = []\n",
    "    for path in imgs_path.iterdir():\n",
    "        name_match = rgb_digit_pattern.match(path.name)\n",
    "        if not name_match:\n",
    "            continue\n",
    "\n",
    "        digit = int(name_match.groupdict()[\"digit\"])\n",
    "        digit_path_pairs.append((digit, path))\n",
    "\n",
    "    return [path for _, path in sorted(digit_path_pairs)]\n",
    "\n",
    "\n",
    "def load_img(\n",
    "    img_path: pl.Path,\n",
    "    downsample_factor: int = 1,\n",
    "    antialias: bool = True,\n",
    "    interpolation: transforms.InterpolationMode = transforms.InterpolationMode.BICUBIC,\n",
    ") -> torch.Tensor:\n",
    "    img = io.read_image(str(img_path), mode=io.ImageReadMode.RGB_ALPHA) / 255.0\n",
    "\n",
    "    if downsample_factor > 1:\n",
    "        C, H, W = img.shape\n",
    "        img = transforms.Resize(\n",
    "            (H // downsample_factor, W // downsample_factor),\n",
    "            antialias=antialias,\n",
    "            interpolation=interpolation,\n",
    "        )(img)\n",
    "        img = torch.clamp(img, 0., 1.)\n",
    "\n",
    "    return img\n",
    "\n",
    "\n",
    "class FrameDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_path: pl.Path,\n",
    "        data_mode: str,  # 'train', 'val', 'test'\n",
    "        ex_idx: int = 0,\n",
    "        downsample_factor: int = 1,\n",
    "        antialias: bool = True,\n",
    "        interpolation: transforms.InterpolationMode = transforms.InterpolationMode.BICUBIC,\n",
    "        seg_cutoff: float = 2e-2 \n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.downsample_factor = downsample_factor\n",
    "\n",
    "        transforms_path = data_path / f\"transforms_{data_mode}.json\"\n",
    "        with open(transforms_path, \"r\") as f:\n",
    "            transforms = json.load(f)\n",
    "        self.cam_angle_x = float(transforms[\"camera_angle_x\"])\n",
    "\n",
    "        self.poses = [\n",
    "            # Assume ordered\n",
    "            torch.FloatTensor(frame[\"transform_matrix\"])\n",
    "            for frame in transforms[\"frames\"]\n",
    "        ]\n",
    "\n",
    "        imgs_path = data_path / data_mode\n",
    "        img_paths = load_img_paths(imgs_path)\n",
    "        imgs = [\n",
    "            load_img(img_path, downsample_factor, antialias, interpolation)\n",
    "            for img_path in img_paths\n",
    "        ]\n",
    "        imgs = torch.stack(imgs)[:, :3]\n",
    "        self.imgs = imgs.permute(0, 2, 3, 1)\n",
    "        self.segs = (self.imgs.sum(axis=-1) > seg_cutoff).float()\n",
    "\n",
    "        self.ex_img, *_ = self[ex_idx]\n",
    "        self.H, self.W, self.C = self.ex_img.shape\n",
    "\n",
    "        self.focal = 0.5 * self.W / np.tan(0.5 * self.cam_angle_x)\n",
    "\n",
    "    @property\n",
    "    def shape(self) -> tuple[int, int]:\n",
    "        return self.H, self.W\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.poses)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        return self.imgs[idx], self.poses[idx], self.segs[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "downscale_factor = 8\n",
    "\n",
    "data_source_name = \"NeRF_Data\"\n",
    "dataset_name = \"nerf_synthetic\"\n",
    "scene_name = \"lego\"\n",
    "\n",
    "root_data_dir = pl.Path('./data/')\n",
    "data_path = root_data_dir / data_source_name / dataset_name / scene_name\n",
    "\n",
    "train_dataset = FrameDataset(data_path, \"train\", downsample_factor=downscale_factor)\n",
    "val_dataset = FrameDataset(data_path, \"val\", downsample_factor=downscale_factor)\n",
    "test_dataset = FrameDataset(data_path, \"test\", downsample_factor=downscale_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#H, W = imgs.shape[1:3]\n",
    "H, W = train_dataset.shape\n",
    "\n",
    "i_ref = 50\n",
    "ref_img, ref_pose, ref_seg = val_dataset[i_ref]\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"Data overview\")\n",
    "print(f\"\\tNumber of samples: {len(train_dataset)}\")\n",
    "print(f\"\\tImage shape: ({train_dataset.shape})\")\n",
    "print(f\"\\tPoses shape: ({ref_img.shape})\")\n",
    "print(f\"\\tFocal value: {train_dataset.focal:.3f}\")\n",
    "print(f\"\\tImage range: ({ref_img.min(), ref_img.max()})\")\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1)\n",
    "ax1.imshow(ref_img)\n",
    "ax2.imshow(ref_seg)\n",
    "print(\"=\" * 50)\n",
    "print(\"Pose:\")\n",
    "print(np.round(ref_pose, 3))\n",
    "\n",
    "train_dataset.imgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class NerfSeg(nn.Module):\n",
    "    def __init__(\n",
    "        self, name: str, L1: int, L2: int, n_hidden: int, act_func: nn.Module = nn.ReLU\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.name = name\n",
    "\n",
    "        self.d1 = 6 * L1 + 3\n",
    "        self.d2 = 6 * L2 + 3\n",
    "\n",
    "        self.stack1 = nn.Sequential(\n",
    "            nn.Linear(self.d1, n_hidden),\n",
    "            act_func(),\n",
    "            nn.Linear(n_hidden, n_hidden),\n",
    "            act_func(),\n",
    "            nn.Linear(n_hidden, n_hidden),\n",
    "            act_func(),\n",
    "            nn.Linear(n_hidden, n_hidden),\n",
    "            act_func(),\n",
    "        )\n",
    "\n",
    "        self.stack2 = nn.Sequential(\n",
    "            nn.Linear(n_hidden + self.d1, n_hidden),\n",
    "            act_func(),\n",
    "            nn.Linear(n_hidden, n_hidden),\n",
    "            act_func(),\n",
    "            nn.Linear(n_hidden, n_hidden),\n",
    "            act_func(),\n",
    "            nn.Linear(n_hidden, n_hidden),\n",
    "            act_func(),\n",
    "            nn.Linear(n_hidden, n_hidden + 2),\n",
    "        )\n",
    "\n",
    "        self.stack3 = nn.Sequential(\n",
    "            nn.Linear(n_hidden + self.d2, n_hidden // 2),\n",
    "            act_func(),\n",
    "            nn.Linear(n_hidden // 2, 3),\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self, o: torch.Tensor, d: torch.Tensor\n",
    "    ) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        x = o\n",
    "\n",
    "        x = self.stack1(x)\n",
    "        x = torch.cat((o, x), dim=-1)\n",
    "\n",
    "        x = self.stack2(x)\n",
    "        sigma = x[..., 0:1]\n",
    "        seg = x[..., 1:2]\n",
    "\n",
    "        x = torch.cat((x[..., 2:], d), dim=-1)\n",
    "        x = self.stack3(x)\n",
    "        rgb = x\n",
    "\n",
    "        y = torch.cat((rgb, sigma, seg), dim=-1)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "\n",
    "\n",
    "class BackpropException(Exception):\n",
    "    ...\n",
    "\n",
    "\n",
    "@torch.jit.script\n",
    "def pos_enc(x: torch.Tensor, L: int) -> torch.Tensor:\n",
    "    # TODO: Optimimze\n",
    "    rets = [x]\n",
    "    for i in range(L):\n",
    "        z = 2.0**i * x\n",
    "        rets.append(torch.sin(z))\n",
    "        rets.append(torch.cos(z))\n",
    "\n",
    "    return torch.cat(rets, -1)\n",
    "\n",
    "\n",
    "@torch.jit.script\n",
    "def positional_encoding(p: torch.Tensor, L: int, device: torch.device) -> torch.Tensor:\n",
    "    assert len(p.shape) == 3\n",
    "    assert L % 2 == 0\n",
    "\n",
    "    B, NB, D = p.shape\n",
    "\n",
    "    pe = torch.zeros(B, NB, 2 * D * L + D, device=device)\n",
    "    pe[..., :3] = p\n",
    "    for i in range(0, L):\n",
    "        pe[..., 3 + 6 * i : 3 + 6 * i + 3] = torch.sin(2**i * p)\n",
    "        pe[..., 3 + 6 * i + 3 : 3 + 6 * i + 6] = torch.cos(2**i * p)\n",
    "    return pe\n",
    "\n",
    "\n",
    "@torch.jit.script\n",
    "def get_rays(\n",
    "    H: int, W: int, focal: float, c2w: torch.Tensor, device: torch.device\n",
    ") -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    x = torch.arange(W, dtype=torch.float32, device=device)\n",
    "    y = torch.arange(H, dtype=torch.float32, device=device)\n",
    "    i, j = torch.meshgrid(x, y, indexing=\"xy\")\n",
    "    dirs = torch.stack(\n",
    "        ((i - W * 0.5) / focal, -(j - H * 0.5) / focal, -torch.ones_like(i)), dim=-1\n",
    "    )\n",
    "    rays_d = torch.sum(dirs[..., None, :] * c2w[:3, :3], -1)\n",
    "    rays_o = c2w[:3, -1].broadcast_to(rays_d.shape).clone()\n",
    "\n",
    "    return rays_o, rays_d\n",
    "\n",
    "\n",
    "@torch.jit.script\n",
    "def exclusive_cumprod(x: torch.Tensor) -> torch.Tensor:\n",
    "    x = torch.cumprod(x, -1)\n",
    "    x = x.roll(1)\n",
    "    x[..., 0] = 1\n",
    "    return x\n",
    "\n",
    "\n",
    "@torch.jit.script\n",
    "def volume_rendering(\n",
    "    rgb: torch.Tensor,\n",
    "    sigma: torch.Tensor,\n",
    "    seg: torch.Tensor,\n",
    "    t: torch.Tensor,\n",
    "    device: torch.device,\n",
    "):\n",
    "    delta = torch.diff(\n",
    "        t,\n",
    "        dim=-1,\n",
    "        append=1e10 * torch.ones(t[..., :1].shape, device=device, dtype=torch.float32),\n",
    "    )\n",
    "    sigma_delta = torch.exp(-sigma * delta)\n",
    "\n",
    "    w = (1.0 - sigma_delta) * exclusive_cumprod(sigma_delta + 1e-10)\n",
    "\n",
    "    rgb_map = torch.sum(w[..., None] * rgb, -2)\n",
    "    depth_map = torch.sum(w * t, -1)\n",
    "    seg_map = torch.sum(w * seg, -1)\n",
    "    seg_map = torch.clip(seg_map, 0.000001, 0.999999)\n",
    "\n",
    "    return rgb_map, depth_map, seg_map\n",
    "\n",
    "\n",
    "def render_rays(\n",
    "    model: nn.Module,\n",
    "    rays_o: torch.Tensor,\n",
    "    rays_d: torch.Tensor,\n",
    "    t_near: float,\n",
    "    t_far: float,\n",
    "    n_samples_per_ray: int,\n",
    "    L1: int,\n",
    "    L2: int,\n",
    "    chunk_size: int,\n",
    "    device: torch.device,\n",
    "    use_stratified_sampling: bool = False,\n",
    "):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        t = (\n",
    "            torch.linspace(t_near, t_far, n_samples_per_ray, device=device)\n",
    "            .broadcast_to([*rays_o.shape[:-1], n_samples_per_ray])\n",
    "            .clone()\n",
    "        )\n",
    "        if use_stratified_sampling:\n",
    "            scalar = (t_far - t_near) / n_samples_per_ray\n",
    "            offsets = torch.rand([*rays_o.shape[:-1], n_samples_per_ray], device=device)\n",
    "            t += offsets * scalar\n",
    "\n",
    "        pts = rays_o[..., None, :] + rays_d[..., None, :] * t[..., :, None]\n",
    "        dirs = rays_d[..., None, :].broadcast_to(pts.shape)\n",
    "\n",
    "        x = pts.reshape(-1, 3)\n",
    "        x = pos_enc(x, L1)\n",
    "        d = dirs.reshape(-1, 3)\n",
    "        d = pos_enc(d, L2)\n",
    "\n",
    "        ys = [\n",
    "            model(x[i : i + chunk_size], d[i : i + chunk_size])\n",
    "            for i in range(0, x.size(0), chunk_size)\n",
    "        ]\n",
    "        y = torch.cat(tuple(ys), dim=0).reshape([*pts.shape[:-1], ys[0].size(-1)])\n",
    "\n",
    "        rgb = y[..., :3]\n",
    "        rgb = nn.functional.sigmoid(rgb)\n",
    "\n",
    "        sigma = y[..., 3]\n",
    "        sigma = nn.functional.relu(sigma)\n",
    "\n",
    "        seg = y[..., 4]\n",
    "        seg = nn.functional.sigmoid(seg)\n",
    "\n",
    "        rgb_map, depth_map, seg_map = volume_rendering(rgb, sigma, seg, t, device)\n",
    "\n",
    "    return rgb_map, depth_map, seg_map\n",
    "\n",
    "\n",
    "def render_rays_and_train(\n",
    "    model: nn.Module,\n",
    "    optim: torch.optim.Optimizer,\n",
    "    rays_o: torch.Tensor,\n",
    "    rays_d: torch.Tensor,\n",
    "    rgb_gt: torch.Tensor,\n",
    "    seg_gt: torch.Tensor,\n",
    "    seg_loss_scalar: float,\n",
    "    t_near: float,\n",
    "    t_far: float,\n",
    "    n_samples_per_ray: int,\n",
    "    grad_clip_val: float,\n",
    "    L1: int,\n",
    "    L2: int,\n",
    "    chunk_size: int,\n",
    "    device: torch.device,\n",
    "    use_stratified_sampling: bool = False,\n",
    "):\n",
    "    H, W, _ = rays_o.shape\n",
    "    assert (H * W) % chunk_size == 0\n",
    "\n",
    "    t = (\n",
    "        torch.linspace(t_near, t_far, n_samples_per_ray, device=device)\n",
    "        .broadcast_to((H, W, n_samples_per_ray))\n",
    "        .clone()\n",
    "    )\n",
    "    if use_stratified_sampling:\n",
    "        t += torch.rand(t.shape, device=device) * (t_far - t_near) / n_samples_per_ray\n",
    "\n",
    "    pts = rays_o[..., None, :] + rays_d[..., None, :] * t[..., :, None]\n",
    "    dirs = rays_d[..., None, :].broadcast_to(pts.shape)\n",
    "\n",
    "    x = pts.reshape(-1, n_samples_per_ray, 3)\n",
    "    x = positional_encoding(x, L1, device)\n",
    "    d = dirs.reshape(-1, n_samples_per_ray, 3)\n",
    "    d = positional_encoding(d, L2, device)\n",
    "\n",
    "    rgb_gt_flat = rgb_gt.reshape(-1, 3)\n",
    "    seg_gt_flat = seg_gt.reshape(-1)\n",
    "    t_flat = t.reshape(-1, n_samples_per_ray)\n",
    "\n",
    "    loss_rgb = 0\n",
    "    loss_seg = 0\n",
    "    loss = 0\n",
    "    rgb_mean = 0\n",
    "\n",
    "    for i in range(0, x.size(0), chunk_size):\n",
    "        optim.zero_grad()\n",
    "\n",
    "        x_chunk = x[i : i + chunk_size]\n",
    "        d_chunk = d[i : i + chunk_size]\n",
    "        rgb_gt_chunk = rgb_gt_flat[i : i + chunk_size]\n",
    "        seg_gt_chunk = seg_gt_flat[i : i + chunk_size]\n",
    "        t_chunk = t_flat[i : i + chunk_size]\n",
    "\n",
    "        y = model(x_chunk, d_chunk).reshape(-1, n_samples_per_ray, 5)\n",
    "\n",
    "        rgb = y[..., :3]\n",
    "        rgb = nn.functional.sigmoid(rgb)\n",
    "\n",
    "        sigma = y[..., 3]\n",
    "        sigma = nn.functional.relu(sigma)\n",
    "\n",
    "        seg = y[..., 4]\n",
    "        seg = nn.functional.sigmoid(seg)\n",
    "\n",
    "        rgb, sigma, seg = volume_rendering(rgb, sigma, seg, t_chunk, device)\n",
    "\n",
    "        loss_rgb_chunk = nn.functional.mse_loss(rgb, rgb_gt_chunk)\n",
    "        loss_seg_chunk = nn.functional.binary_cross_entropy(seg, seg_gt_chunk)\n",
    "\n",
    "        loss_chunk = loss_rgb_chunk + seg_loss_scalar * loss_seg_chunk\n",
    "        try:\n",
    "            loss_chunk.backward()\n",
    "        except Exception as e:\n",
    "            raise BackpropException(\n",
    "                f\"rgb_shape: {rgb.shape}, sigma_shape: {sigma.shape}, seg_shape: {seg.shape}, loss_rgb: {loss_rgb}, seg_range: ({seg.min()},{seg.max()}), loss_seg: {loss_seg}, loss: {loss}\",\n",
    "                e,\n",
    "            )\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip_val)\n",
    "        optim.step()\n",
    "\n",
    "        loss_rgb += loss_rgb_chunk.item()\n",
    "        loss_seg += loss_seg_chunk.item()\n",
    "        loss += loss_chunk.item()\n",
    "        rgb_mean += rgb.mean().item()\n",
    "\n",
    "    n_chunks = H * W // chunk_size\n",
    "    loss_rgb /= n_chunks\n",
    "    loss_seg /= n_chunks\n",
    "    loss /= n_chunks\n",
    "    rgb_mean /= n_chunks\n",
    "\n",
    "    return loss_rgb, loss_seg, loss, rgb_mean\n",
    "\n",
    "\n",
    "def save_rgb_tensor_as_img(x: torch.Tensor, path: pl.Path):\n",
    "    x = x.detach().cpu().permute(2, 0, 1)\n",
    "    im = torchvision.transforms.ToPILImage(\"RGB\")(x)\n",
    "    im.save(path)\n",
    "\n",
    "\n",
    "def save_mono_tensor_as_img(x: torch.Tensor, path: pl.Path):\n",
    "    x = x.detach().cpu()\n",
    "    im = torchvision.transforms.ToPILImage()(x)\n",
    "    im.save(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import shutil\n",
    "\n",
    "\n",
    "def train_nerf(\n",
    "    model: nn.Module,\n",
    "    optim: torch.optim.Optimizer,\n",
    "    n_iter: int,\n",
    "    base_model_path: pl.Path,\n",
    "    train_dataset: Dataset,\n",
    "    ref_sample: tuple[torch.Tensor, torch.Tensor, torch.Tensor],\n",
    "    device: torch.device,\n",
    "    t_near: float,\n",
    "    t_far: float,\n",
    "    d_pos_embed: int,\n",
    "    d_dir_embed: int,\n",
    "    n_samples_per_ray: int,\n",
    "    chunk_size: int,\n",
    "    grad_clip_val: float,\n",
    "    load_checkpoint_path: pl.Path = None,\n",
    "    save_freq: int = 20,\n",
    "    print_epoch: bool = True,\n",
    "):\n",
    "    if not base_model_path.exists():\n",
    "        raise ValueError\n",
    "\n",
    "    if load_checkpoint_path:\n",
    "        if not load_checkpoint_path.exists():\n",
    "            raise ValueError\n",
    "\n",
    "        checkpoint = torch.load(str(load_checkpoint_path))\n",
    "        model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "        optim.load_state_dict(checkpoint[\"optim_state_dict\"])\n",
    "        i_sample = checkpoint[\"epochs\"][-1] + 1\n",
    "\n",
    "    else:\n",
    "        checkpoint = {\n",
    "            \"epochs\": [],\n",
    "            \"train_loss\": [],\n",
    "            \"train_loss_rgb\": [],\n",
    "            \"train_loss_seg\": [],\n",
    "            \"ref_loss\": [],\n",
    "            \"ref_loss_rgb\": [],\n",
    "            \"ref_loss_seg\": [],\n",
    "            \"model_state_dict\": None,\n",
    "            \"optim_state_dict\": None,\n",
    "        }\n",
    "        i_sample = 0\n",
    "\n",
    "    model_save_path = base_model_path / model.name\n",
    "    model_save_path.mkdir(exist_ok=True)\n",
    "\n",
    "    version = len(list(model_save_path.iterdir()))\n",
    "    version_path = model_save_path / f\"version-{version}\"\n",
    "    version_path.mkdir(exist_ok=False)\n",
    "\n",
    "    if version_path.exists():\n",
    "        shutil.rmtree(version_path)\n",
    "\n",
    "    if not version_path.exists():\n",
    "        version_path.mkdir()\n",
    "\n",
    "    ref_rgb, ref_pose, ref_seg = ref_sample\n",
    "\n",
    "    ref_path = version_path / \"ref\"\n",
    "    ref_path.mkdir()\n",
    "    save_rgb_tensor_as_img(ref_rgb, ref_path / f\"rgb.png\")\n",
    "    save_mono_tensor_as_img(ref_seg, ref_path / f\"cls.png\")\n",
    "\n",
    "    ref_rgb = ref_rgb.to(device)\n",
    "    ref_pose = ref_pose.to(device)\n",
    "    ref_seg = ref_seg.to(device)\n",
    "\n",
    "    H, W = train_dataset.shape\n",
    "\n",
    "    ref_rays_o, ref_rays_d = get_rays(\n",
    "        H, W, train_dataset.focal, ref_pose, device=device\n",
    "    )\n",
    "\n",
    "    prev_time = time.time()\n",
    "    while i_sample <= n_iter:\n",
    "        i = np.random.randint(len(train_dataset))\n",
    "\n",
    "        with torch.no_grad():\n",
    "            rgb_gt, pose, seg_gt = train_dataset[i]\n",
    "            rgb_gt = rgb_gt.to(device)\n",
    "            pose = pose.to(device)\n",
    "            seg_gt = seg_gt.to(device)\n",
    "\n",
    "            rays_o, rays_d = get_rays(H, W, train_dataset.focal, pose, device=device)\n",
    "\n",
    "        model.train()\n",
    "        # try:\n",
    "        loss_rgb, loss_seg, loss, rgb_mean = render_rays_and_train(\n",
    "            model,\n",
    "            optim,\n",
    "            rays_o,\n",
    "            rays_d,\n",
    "            rgb_gt,\n",
    "            seg_gt,\n",
    "            seg_loss_scalar,\n",
    "            t_near,\n",
    "            t_far,\n",
    "            n_samples_per_ray,\n",
    "            grad_clip_val,\n",
    "            d_pos_embed,\n",
    "            d_dir_embed,\n",
    "            chunk_size,\n",
    "            device=device,\n",
    "            use_stratified_sampling=use_stratified_sampling,\n",
    "        )\n",
    "        # except Exception as e:\n",
    "        #    print(f\"Epoch {i_sample} - Failed on backprop due to exception\", e)\n",
    "        #    i_sample += 1\n",
    "        #    continue\n",
    "\n",
    "        if i_sample % save_freq == 0:\n",
    "            curr_time = time.time()\n",
    "            model.eval()\n",
    "\n",
    "            rgb_rp, depth_rp, seg_rp = render_rays(\n",
    "                model,\n",
    "                ref_rays_o,\n",
    "                ref_rays_d,\n",
    "                t_near,\n",
    "                t_far,\n",
    "                n_samples_per_ray,\n",
    "                d_pos_embed,\n",
    "                d_dir_embed,\n",
    "                chunk_size,\n",
    "                device=device,\n",
    "                use_stratified_sampling=use_stratified_sampling,\n",
    "            )\n",
    "            ref_loss_rgb = nn.functional.mse_loss(rgb_rp, ref_rgb)\n",
    "            ref_loss_seg = nn.functional.binary_cross_entropy(seg_rp, ref_seg)\n",
    "            ref_loss = ref_loss_rgb + seg_loss_scalar * ref_loss_seg\n",
    "\n",
    "            epoch_path = version_path / f\"epoch-{i_sample}\"\n",
    "            epoch_path.mkdir()\n",
    "            checkpoint[\"epochs\"].append(i_sample)\n",
    "            checkpoint[\"train_loss\"].append(loss)\n",
    "            checkpoint[\"train_loss_rgb\"].append(loss_rgb)\n",
    "            checkpoint[\"train_loss_seg\"].append(loss_seg)\n",
    "            checkpoint[\"ref_loss\"].append(ref_loss)\n",
    "            checkpoint[\"ref_loss_rgb\"].append(ref_loss_rgb)\n",
    "            checkpoint[\"ref_loss_seg\"].append(ref_loss_seg)\n",
    "            checkpoint[\"model_state_dict\"] = model.state_dict()\n",
    "            checkpoint[\"optim_state_dict\"] = optim.state_dict()\n",
    "            torch.save(checkpoint, epoch_path / f\"checkpoint.cp\")\n",
    "\n",
    "            save_rgb_tensor_as_img(rgb_rp, epoch_path / f\"rgb.png\")\n",
    "            save_mono_tensor_as_img(depth_rp, epoch_path / f\"depth.png\")\n",
    "            save_mono_tensor_as_img(seg_rp, epoch_path / f\"cls.png\")\n",
    "\n",
    "            if print_epoch:\n",
    "                log_path = version_path / \"training.log\"\n",
    "                with open(log_path, \"a\") as f:\n",
    "                    f.writelines(\n",
    "                        [\n",
    "                            \"\\n\",\n",
    "                            \"=\" * 50 + '\\n',\n",
    "                            f\"Epoch: {i_sample} / {n_iter}\\n\",\n",
    "                            f\"Duration / Epoch: {(curr_time - prev_time)/save_freq:.3f}\\n\",\n",
    "                            f\"Train loss: {loss:.5e}\\n\",\n",
    "                            f\"Train loss - RGB: {loss_rgb:.5e}\\n\",\n",
    "                            f\"Train loss - Segmentation: {loss_seg:.5e}\\n\",\n",
    "                            f\"Ref loss: {ref_loss:.5e}\\n\",\n",
    "                            f\"Ref loss - RGB: {ref_loss_rgb:.5e}\\n\",\n",
    "                            f\"Ref loss - Segmentation: {ref_loss_seg:.5e}\\n\",\n",
    "                            f\"Mean outout: {rgb_mean:.5f}\\n\",\n",
    "                            \"\\n\"\n",
    "                        ]\n",
    "                    )\n",
    "\n",
    "            prev_time = time.time()\n",
    "\n",
    "        i_sample += 1\n",
    "\n",
    "\n",
    "\n",
    "n_samples_per_ray = 128\n",
    "n_hidden = 256\n",
    "d_pos_embed = 10\n",
    "d_dir_embed = 4\n",
    "t_near = 2.0\n",
    "t_far = 6.0\n",
    "\n",
    "lr = 3e-4\n",
    "chunk_size = 1000\n",
    "grad_clip_val = 2.0\n",
    "seg_loss_scalar = 5e-4\n",
    "n_iter = 2000\n",
    "\n",
    "use_stratified_sampling = True\n",
    "save_freq = 20\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_name = \"nerf-seg\"\n",
    "model = NerfSeg(\n",
    "    model_name, L1=d_pos_embed, L2=d_dir_embed, n_hidden=n_hidden, act_func=nn.ReLU\n",
    ").to(device)\n",
    "optim = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "base_model_path = pl.Path(\"./models\")\n",
    "\n",
    "\n",
    "\n",
    "TRAIN_MODEL = True\n",
    "if TRAIN_MODEL:\n",
    "    train_nerf(\n",
    "        model=model,\n",
    "        optim=optim,\n",
    "        n_iter=n_iter,\n",
    "        base_model_path=base_model_path,\n",
    "        train_dataset=train_dataset,\n",
    "        ref_sample=(ref_img, ref_pose, ref_seg),\n",
    "        device=device,\n",
    "        t_near=t_near,\n",
    "        t_far=t_far,\n",
    "        d_pos_embed=d_pos_embed,\n",
    "        d_dir_embed=d_dir_embed,\n",
    "        n_samples_per_ray=n_samples_per_ray,\n",
    "        chunk_size=chunk_size,\n",
    "        grad_clip_val=grad_clip_val,\n",
    "        save_freq=save_freq,\n",
    "        load_checkpoint_path=pl.Path('models/nerf-seg/version-3/epoch-760/checkpoint.cp')\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pose_to_image(\n",
    "    pose, H, W, focal, t_near, t_far, n_samples, L1, chunk_size, model, device\n",
    "):\n",
    "    with torch.no_grad():\n",
    "        rays_o, rays_d = get_rays(\n",
    "            H, W, focal, torch.from_numpy(pose).to(device), device=device\n",
    "        )\n",
    "        rgb, depth, cls = render_rays(\n",
    "            model,\n",
    "            rays_o,\n",
    "            rays_d,\n",
    "            t_near,\n",
    "            t_far,\n",
    "            n_samples,\n",
    "            L1,\n",
    "            chunk_size,\n",
    "            device=device,\n",
    "            use_stratified_sampling=use_stratified_sampling,\n",
    "        )\n",
    "    return rgb.detach().cpu().numpy()\n",
    "\n",
    "\n",
    "def transform_pose(pose: np.ndarray, t: np.ndarray=0, rx: np.ndarray=0, ry: np.ndarray=0, rz: np.ndarray=0):\n",
    "    pose = np.copy(pose)\n",
    "    pose[:3, 0] += np.array(rx)\n",
    "    pose[:3, 1] += np.array(ry)\n",
    "    pose[:3, 2] += np.array(rz)\n",
    "    pose[:3, 3] += np.array(t)\n",
    "    return pose\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "model = NerfSeg(\"tiny-nerf-seg\", d_pos_embed, n_hidden).to(device)\n",
    "cp = torch.load(\"./saved-models/tiny-nerf-seg_version-42/epoch-950/checkpoint.cp\")\n",
    "model.load_state_dict(cp[\"model_state_dict\"])\n",
    "model.eval()\n",
    "\n",
    "pose = transform_pose(ref_pose, t=(0, 0, 0), rx=(0, 0, 0))\n",
    "ex_img = pose_to_image(\n",
    "    pose, H, W, focal, t_near, t_far, n_samples_per_ray, d_pos_embed, chunk_size, model, device\n",
    ")\n",
    "plt.imshow(ex_img)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
