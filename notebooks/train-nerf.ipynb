{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from src.data import FrameDataset\n",
    "from src.data import RayDataset\n",
    "\n",
    "import pathlib as pl\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "\n",
    "\n",
    "def get_path_number(path: pl.Path):\n",
    "    return int(path.stem.split(\"-\")[-1])\n",
    "\n",
    "\n",
    "@torch.jit.script\n",
    "def strat_sampling(\n",
    "    N: int, t_near: float, t_far: float, device: torch.device\n",
    ") -> torch.Tensor:\n",
    "    samples = (\n",
    "        (torch.arange(N, device=device) + torch.rand(N, device=device))\n",
    "        * (t_far - t_near)\n",
    "        / N\n",
    "    )  # <N>\n",
    "    return samples\n",
    "\n",
    "\n",
    "@torch.jit.script\n",
    "def positional_encoding(p: torch.Tensor, L: int, device: torch.device) -> torch.Tensor:\n",
    "    assert len(p.shape) == 3\n",
    "    B, NB, D = p.shape\n",
    "\n",
    "    # Z denotes transformed input p\n",
    "    # Z_ij becomes 2^i * p_i * p_j for each i in 0..L-1 and each component j in 1..3\n",
    "    # Thus dimension is <B, NB, D, L>\n",
    "    z = (2 ** torch.arange(L, device=device).repeat(D, 1)) * (torch.pi * p[..., None])\n",
    "\n",
    "    # X denotes the encoded value for each transformed input\n",
    "    x1 = torch.sin(z)\n",
    "    x2 = torch.cos(z)\n",
    "\n",
    "    # We want ordering sin(x) cos(x) sin(y) cos(y) sin(z) cos(z) repeated for each element in 1..L\n",
    "    # First we stack encoding into a matrix, then we flatten the matrix to put each row side by side.\n",
    "    x = torch.stack((x1, x2), dim=4)  # <B, NB, D, L, 2>\n",
    "    x = x.swapaxes(2, 3)  # <B, NB, L, D, 2>\n",
    "    x = x.reshape(B, NB, 2 * D * L)  # Finally, flatten to shape <B, NB, 2*D*L>\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "@torch.jit.script\n",
    "def get_t(\n",
    "    n_rays: int, n_bins: int, t_near: float, t_far: float, device: torch.device\n",
    ") -> torch.Tensor:\n",
    "    t = strat_sampling(n_rays * n_bins, t_near, t_far, device)\n",
    "    t = t.reshape(n_rays, n_bins)  # B, NB\n",
    "    return t\n",
    "\n",
    "\n",
    "@torch.jit.script\n",
    "def batchify_rays(batch: tuple[torch.Tensor, torch.Tensor, torch.Tensor]):\n",
    "    # r_o: <S, S, D>\n",
    "    # r_d: <S, S, D>\n",
    "    # C_r: <4, S, S>\n",
    "    # img_size: S\n",
    "    # n_rays: N = S*S\n",
    "    # N = S * S\n",
    "    # chunk_size: C = (S/NC)^2\n",
    "\n",
    "    r_o, r_d, C_r = batch\n",
    "    S, _, D = r_o.shape\n",
    "    r_o = r_o.reshape(S * S, D)\n",
    "    r_d = r_d.reshape(S * S, D)\n",
    "\n",
    "    # We only care about RGB, which is the first three dimensions\n",
    "    # We also reshape it to <N, 3> to make it an RGB value for each ray\n",
    "    C_r = C_r[:3].reshape(3, -1).T  # <N, D>\n",
    "\n",
    "    # r_o, r_d: <N, D>\n",
    "    # C_r: <N, 3>\n",
    "    return r_o, r_d, C_r\n",
    "\n",
    "\n",
    "@torch.jit.script\n",
    "def preprocess_rays(\n",
    "    r_o: torch.Tensor,\n",
    "    r_d: torch.Tensor,\n",
    "    n_bins: int,\n",
    "    t_near: float,\n",
    "    t_far: float,\n",
    "    L1: int,\n",
    "    L2: int,\n",
    "    device: torch.device,\n",
    "):\n",
    "    # r_o: <N, D>\n",
    "    # r_o: <N, D>\n",
    "    # C_r: <N, 3>\n",
    "\n",
    "    N, D = r_o.shape\n",
    "\n",
    "    t = get_t(N, n_bins, t_near, t_far, device=device)  # <C, NB>\n",
    "    r_d = nn.functional.normalize(r_d, dim=-1)  # <N, D>\n",
    "\n",
    "    # Reshape the dimensions for broadcasting during x = r_o + t * r_d\n",
    "    r_o = r_o.reshape(N, 1, D)\n",
    "\n",
    "    # Repeat this for broadcasting when multiplying\n",
    "    r_d = r_d.reshape(N, 1, D).repeat(1, n_bins, 1)\n",
    "\n",
    "    # We will do elementwise multiplication with each dimension, so we add a dimension at the end for broadcasting\n",
    "    tmul = t[..., None]\n",
    "    x = r_o + tmul * r_d  # <N, NB, D>\n",
    "\n",
    "    # <N, NB, 2*D*L>\n",
    "    ex = positional_encoding(x, L1, device=device)\n",
    "    ed = positional_encoding(r_d, L2, device=device)\n",
    "\n",
    "    ex = ex.reshape(N, n_bins, 6 * L1)  # <C, NB, 2*D*L>\n",
    "    ed = ed.reshape(N, n_bins, 6 * L2)  # <C, NB, 2*D*L>\n",
    "\n",
    "    return ex, ed, t\n",
    "\n",
    "\n",
    "@torch.jit.script\n",
    "def expected_color(\n",
    "    c: torch.Tensor, sigma: torch.Tensor, t: torch.Tensor\n",
    ") -> torch.Tensor:\n",
    "    # N: number of rays in batch\n",
    "    # NB: number of bins\n",
    "    # c: <N, NB, 3>\n",
    "    # sigma: <N, NB>\n",
    "    # t: <N, NB>\n",
    "\n",
    "    assert len(c.shape) == 3\n",
    "    assert len(sigma.shape) == 2\n",
    "    assert len(t.shape) == 2\n",
    "\n",
    "\n",
    "    # Multiply up to T-1. For effeciency purposes, we keep the last dimension, but it gets overwritten later.\n",
    "    dt = torch.roll(t, -1, dims=-1) - t\n",
    "    mul = dt * sigma  \n",
    "\n",
    "    # Compute cumuluative probability,\n",
    "    # Since equation (3) sums T_i from i=1 to i-1, we set the first value to (exp 0 = 1) and ignore the last value.\n",
    "    # We don't remove it from the tensor yet, but it will be overwritten later.\n",
    "    T = torch.exp(-torch.cumsum(mul, dim=-1))\n",
    "    T = T.roll(1, dims=-1)\n",
    "    T[..., 0] = 1\n",
    "\n",
    "    # Since we do no have a delta for the last value,\n",
    "    # we directly set the last value of w to T at i=N,\n",
    "    # which is the dot product between sigma and delta\n",
    "    w = T * (1 - torch.exp(-mul))\n",
    "    w[..., -1] = torch.einsum(\"nb,nb->n\", dt[..., :-1], sigma[..., :-1])\n",
    "\n",
    "    c_hat = torch.einsum(\"nb,nbc->nc\", w, c)\n",
    "    return c_hat\n",
    "\n",
    "def train_nerf_batch(\n",
    "    batch: tuple[torch.Tensor, torch.Tensor, torch.Tensor],\n",
    "    model: nn.Module,\n",
    "    criterion: nn.Module,\n",
    "    optim: torch.optim.Optimizer,\n",
    "    chunk_size: int,\n",
    "    n_bins: int,\n",
    "    t_near: float,\n",
    "    t_far: float,\n",
    "    L1: int,\n",
    "    L2: int,\n",
    "    device: torch.device,\n",
    ") -> dict[str, any]:\n",
    "    img_size = batch[0].size(0)\n",
    "    n_rays = img_size * img_size\n",
    "    assert n_rays % chunk_size == 0\n",
    "\n",
    "    r_o_full, r_d_full, C_r_full = batchify_rays(batch)\n",
    "\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "\n",
    "    for i in range(0, n_rays, chunk_size):\n",
    "        optim.zero_grad()\n",
    "\n",
    "        r_o = r_o_full[i : i + chunk_size].to(device)\n",
    "        r_d = r_d_full[i : i + chunk_size].to(device)\n",
    "        C_r = C_r_full[i : i + chunk_size].to(device)\n",
    "\n",
    "        ex, ed, t = preprocess_rays(\n",
    "            r_o, r_d, n_bins, t_near, t_far, L1, L2, device=device\n",
    "        )\n",
    "\n",
    "        c, sigma = model(ex, ed)\n",
    "        c_hat = expected_color(c, sigma, t)\n",
    "\n",
    "        batch_loss = criterion(c_hat, C_r)\n",
    "        batch_loss.backward()\n",
    "\n",
    "        optim.step()\n",
    "\n",
    "        running_loss += batch_loss.item()\n",
    "\n",
    "    train_loss = running_loss / (n_rays / chunk_size)\n",
    "    return train_loss\n",
    "\n",
    "\n",
    "def eval_nerf_batch(\n",
    "    batch: tuple[torch.Tensor, torch.Tensor, torch.Tensor],\n",
    "    model: nn.Module,\n",
    "    criterion: nn.Module,\n",
    "    chunk_size: int,\n",
    "    n_bins: int,\n",
    "    t_near: float,\n",
    "    t_far: float,\n",
    "    L1: int,\n",
    "    L2: int,\n",
    "    device: torch.device,\n",
    "):\n",
    "    img_size = batch[0].size(0)\n",
    "    n_rays = img_size * img_size\n",
    "    assert n_rays % chunk_size == 0\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        img_size = batch[0].size(0)\n",
    "        n_rays = img_size * img_size\n",
    "        assert n_rays % chunk_size == 0\n",
    "\n",
    "        r_o_full, r_d_full, C_r_full = batchify_rays(batch)\n",
    "        running_loss = 0\n",
    "\n",
    "        for i in range(0, n_rays, chunk_size):\n",
    "            r_o = r_o_full[i : i + chunk_size].to(device)\n",
    "            r_d = r_d_full[i : i + chunk_size].to(device)\n",
    "            C_r = C_r_full[i : i + chunk_size].to(device)\n",
    "\n",
    "            ex, ed, t = preprocess_rays(\n",
    "                r_o, r_d, n_bins, t_near, t_far, L1, L2, device=device\n",
    "            )\n",
    "\n",
    "            c, sigma = model(ex, ed)\n",
    "            c_hat = expected_color(c, sigma, t)\n",
    "\n",
    "            batch_loss = criterion(c_hat, C_r)\n",
    "            running_loss += batch_loss.item()\n",
    "\n",
    "    val_loss = running_loss / (n_rays / chunk_size)\n",
    "    return val_loss\n",
    "\n",
    "\n",
    "def train_nerf(\n",
    "    model: nn.Module,\n",
    "    optim: torch.optim.Optimizer,\n",
    "    criterion: nn.Module,\n",
    "    train_dataset,\n",
    "    val_dataset,\n",
    "    n_epochs: int,\n",
    "    chunk_size: int,\n",
    "    n_bins: int,\n",
    "    t_near: float,\n",
    "    t_far: float,\n",
    "    L1: int,\n",
    "    L2: int,\n",
    "    base_save_path: pl.Path,\n",
    "    device: torch.device,\n",
    "    load_checkpoint_path: pl.Path = None,\n",
    "    shuffle_train: bool = True,\n",
    "    shuffle_val: bool = True,\n",
    "    limit_train_size: int = None,\n",
    "    limit_val_size: int = None,\n",
    "):\n",
    "    if not base_save_path.exists():\n",
    "        raise ValueError\n",
    "\n",
    "    if load_checkpoint_path:\n",
    "        if not load_checkpoint_path.exists():\n",
    "            raise ValueError\n",
    "\n",
    "        checkpoint = torch.load(str(load_checkpoint_path))\n",
    "        model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "        optim.load_state_dict(checkpoint[\"optim_state_dict\"])\n",
    "        epoch = checkpoint[\"epoch\"] + 1\n",
    "\n",
    "    else:\n",
    "        checkpoint = {\n",
    "            \"epoch\": None,\n",
    "            \"train_loss\": [],\n",
    "            \"val_loss\": [],\n",
    "            \"model_state_dict\": None,\n",
    "            \"optim_state_dict\": None,\n",
    "        }\n",
    "        epoch = 0\n",
    "\n",
    "    model_save_path = base_save_path / model.name\n",
    "    model_save_path.mkdir(exist_ok=True)\n",
    "\n",
    "    n_saved_versions = len(list(model_save_path.glob(\"version-*\")))\n",
    "    if n_saved_versions > 0:\n",
    "        n_saved_versions = (\n",
    "            get_path_number(\n",
    "                sorted(\n",
    "                    model_save_path.glob(\"version-*\"),\n",
    "                    key=get_path_number,\n",
    "                )[-1]\n",
    "            )\n",
    "            + 1\n",
    "        )\n",
    "\n",
    "    version_path = model_save_path / f\"version-{n_saved_versions}\"\n",
    "    version_path.mkdir(parents=True, exist_ok=False)\n",
    "\n",
    "    while epoch < n_epochs:\n",
    "        print(f\"epoch: {epoch}/{n_epochs}\")\n",
    "        train_loss_epoch = 0\n",
    "        n_batches_train = (\n",
    "            len(train_dataset) if (limit_train_size is None) else limit_train_size\n",
    "        )\n",
    "        batch_idxs_train = (\n",
    "            torch.randperm(n_batches_train)\n",
    "            if shuffle_train\n",
    "            else torch.arange(n_batches_train)\n",
    "        )\n",
    "        for i_batch_train in tqdm.tqdm(batch_idxs_train):\n",
    "            batch = train_dataset[i_batch_train]\n",
    "            train_loss_batch = train_nerf_batch(\n",
    "                batch,\n",
    "                model,\n",
    "                criterion,\n",
    "                optim,\n",
    "                chunk_size,\n",
    "                n_bins,\n",
    "                t_near,\n",
    "                t_far,\n",
    "                L1,\n",
    "                L2,\n",
    "                device=device,\n",
    "            )\n",
    "            train_loss_epoch += train_loss_batch\n",
    "\n",
    "        val_loss_epoch = 0\n",
    "        n_batches_val = len(val_dataset) if (limit_val_size is None) else limit_val_size\n",
    "        batch_idxs_val = (\n",
    "            torch.randperm(n_batches_val)\n",
    "            if shuffle_val\n",
    "            else torch.arange(n_batches_val)\n",
    "        )\n",
    "        for i_batch_val in tqdm.tqdm(batch_idxs_val):\n",
    "            batch = val_dataset[i_batch_val]\n",
    "            val_loss_batch = eval_nerf_batch(\n",
    "                batch,\n",
    "                model,\n",
    "                criterion,\n",
    "                chunk_size,\n",
    "                n_bins,\n",
    "                t_near,\n",
    "                t_far,\n",
    "                L1,\n",
    "                L2,\n",
    "                device=device,\n",
    "            )\n",
    "            val_loss_epoch += val_loss_batch\n",
    "\n",
    "        train_loss = train_loss_epoch / n_batches_train\n",
    "        val_loss = val_loss_epoch / n_batches_val\n",
    "\n",
    "        checkpoint_path = version_path / f\"checkpoint-{epoch}\"\n",
    "        print(f\"\\ttrain loss: {train_loss:.5e}\\n\\tval loss: {val_loss:.5e}\\n\")\n",
    "\n",
    "        checkpoint[\"epoch\"] = epoch \n",
    "        checkpoint[\"train_loss\"].append(train_loss)\n",
    "        checkpoint[\"val_loss\"].append(val_loss)\n",
    "        checkpoint[\"model_state_dict\"] = model.state_dict()\n",
    "        checkpoint[\"optim_state_dict\"] = optim.state_dict()\n",
    "        torch.save(checkpoint, checkpoint_path)\n",
    "\n",
    "        epoch += 1\n",
    "\n",
    "    return checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_source_name = \"NeRF_Data\"\n",
    "dataset_name = \"nerf_synthetic\"\n",
    "scene_name = \"lego\"\n",
    "\n",
    "root_data_dir = pl.Path('./data/')\n",
    "data_path = root_data_dir / data_source_name / dataset_name / scene_name\n",
    "\n",
    "train_frame_dataset = FrameDataset(data_path, \"train\", downsample_factor=4)\n",
    "val_frame_dataset = FrameDataset(data_path, \"val\", downsample_factor=4)\n",
    "train_ray_dataset = RayDataset(train_frame_dataset)\n",
    "val_ray_dataset = RayDataset(val_frame_dataset)\n",
    "\n",
    "print(train_frame_dataset.ex_img.shape)\n",
    "print(train_frame_dataset.ex_img[0].max(), train_frame_dataset.ex_img[3].max())\n",
    "plt.imshow(train_frame_dataset.ex_img.T.swapaxes(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model import MediumNet\n",
    "\n",
    "chunk_size = 8000\n",
    "n_epochs = 100\n",
    "n_bins = 100\n",
    "t_near = 0.1\n",
    "t_far = 5.0\n",
    "n_hidden = 256\n",
    "n_components = 3\n",
    "learning_rate = 2e-4\n",
    "L1 = 10\n",
    "L2 = 4\n",
    "\n",
    "base_save_path = pl.Path('./models/')\n",
    "load_checkpoint_path = None\n",
    "\n",
    "# Set these when you want to test. \n",
    "# If None, the full dataset will be used on each epoch.\n",
    "limit_train_size = None\n",
    "limit_val_size = 30\n",
    "\n",
    "\n",
    "#model = TestNet(\"testnet\", L1, L2, n_components, n_hidden).to(DEVICE)\n",
    "model = MediumNet(\"mediumnet\", L1, L2, n_components, n_hidden).to(DEVICE)\n",
    "model = torch.jit.script(model)\n",
    "optim = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "\n",
    "train_nerf(\n",
    "    model=model,\n",
    "    optim=optim,\n",
    "    criterion=criterion,\n",
    "    train_dataset=train_ray_dataset,\n",
    "    val_dataset=val_ray_dataset,\n",
    "    n_epochs=n_epochs,\n",
    "    chunk_size=chunk_size,\n",
    "    n_bins=n_bins,\n",
    "    t_near=t_near,\n",
    "    t_far=t_far,\n",
    "    L1=L1,\n",
    "    L2=L2,\n",
    "    base_save_path=base_save_path,\n",
    "    device=DEVICE,\n",
    "    load_checkpoint_path=load_checkpoint_path,\n",
    "    limit_train_size=limit_train_size,\n",
    "    limit_val_size=limit_val_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp = torch.load(pl.Path('./models/mediumnet-v1/version-0/checkpoint-99'))\n",
    "train_loss = cp[\"train_loss\"]\n",
    "val_loss = cp[\"val_loss\"]\n",
    "n_epochs = cp[\"epoch\"] + 1\n",
    "\n",
    "plt.plot(np.arange(n_epochs), train_loss, label=\"train loss\")\n",
    "plt.plot(np.arange(n_epochs), val_loss, label=\"val loss\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
