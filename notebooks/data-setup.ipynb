{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib as pl\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import io\n",
    "import json\n",
    "\n",
    "data_source_name = \"NeRF_Data\"\n",
    "dataset_name = \"nerf_synthetic\"\n",
    "scene_name = \"lego\"\n",
    "\n",
    "root_data_dir = pl.Path('./data/')\n",
    "data_path = root_data_dir / data_source_name / dataset_name / scene_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "digit_pattern = re.compile(r\"\\d+\")\n",
    "\n",
    "\n",
    "def load_transforms(tfs_path: pl.Path) -> tuple[float, list[float], list[torch.FloatTensor]]:\n",
    "    with open(tfs_path, \"r\") as f:\n",
    "        transforms = json.load(f)\n",
    "\n",
    "    cam_angle_x = float(transforms[\"camera_angle_x\"])\n",
    "    rotations = []\n",
    "    transform_matrixes = []\n",
    "\n",
    "    for frame in transforms[\"frames\"]:       \n",
    "        # Assume ordered\n",
    "        rotation = float(frame[\"rotation\"])\n",
    "        transform_matrix = torch.FloatTensor(frame[\"transform_matrix\"])\n",
    "\n",
    "        rotations.append(rotation)\n",
    "        transform_matrixes.append(transform_matrix)\n",
    "\n",
    "    return cam_angle_x, rotations, transform_matrixes\n",
    "        \n",
    "\n",
    "def extract_digit_from_path_name(path: pl.Path) -> int:\n",
    "    match = digit_pattern.search(path.name)\n",
    "\n",
    "    if not match:\n",
    "        return None\n",
    "\n",
    "    return int(match.group(0))\n",
    "\n",
    "\n",
    "def load_img_paths(imgs_path: pl.Path):\n",
    "    paths = imgs_path.iterdir() # Ordered lexagraphically\n",
    "    paths = sorted(paths, key=extract_digit_from_path_name) # Ordered numerically\n",
    "    return paths\n",
    "\n",
    "\n",
    "def load_frame(\n",
    "        imgs_path: pl.Path,\n",
    "        rotations: list[float],\n",
    "        tf_matrixes: list[torch.FloatTensor],\n",
    "        idx: int\n",
    "    ) -> torch.Tensor:\n",
    "    img_path = imgs_path[idx]\n",
    "    rotation = rotations[idx]\n",
    "    tf_matrix = tf_matrixes[idx]\n",
    "    img = io.read_image(str(img_path), mode=io.ImageReadMode.RGB_ALPHA)\n",
    "\n",
    "    return img, rotation, tf_matrix\n",
    "\n",
    "\n",
    "class FrameDataset(Dataset):\n",
    "    def __init__(\n",
    "            self,\n",
    "            data_path: pl.Path,\n",
    "            data_mode: str,  # 'train', 'val', 'test'\n",
    "            ex_idx: int = 5\n",
    "        ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.imgs_path = data_path / data_mode\n",
    "        self.tfs_path = data_path / f\"transforms_{data_mode}.json\"\n",
    "\n",
    "        self.cam_angle_x, self.rotations, self.transform_matrixes = load_transforms(self.tfs_path)\n",
    "        self.img_paths = load_img_paths(self.imgs_path)\n",
    "\n",
    "        self.ex_img, *_ = self[ex_idx]\n",
    "        self.C, self.H, self.W = self.ex_img.shape\n",
    "\n",
    "        self.focal = 0.5 * self.W / np.tan(0.5 * self.cam_angle_x)\n",
    "\n",
    "\n",
    "    @property\n",
    "    def shape(self) -> tuple[float, float]:\n",
    "        return self.H, self.W\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tfs_path)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        return load_frame(self.img_paths, self.rotations, self.transform_matrixes, idx)\n",
    "\n",
    "\n",
    "train_dataset = FrameDataset(data_path, \"train\")\n",
    "\n",
    "print(train_dataset.ex_img.shape)\n",
    "print(train_dataset.ex_img[0].max(), train_dataset.ex_img[3].max())\n",
    "plt.imshow(train_dataset.ex_img.T.swapaxes(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rays(\n",
    "    H: int, W: int, focal: float, c2w: torch.Tensor\n",
    ") -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    # Ported from https://github.com/bmild/nerf/blob/master/run_nerf_helpers.py#L123\n",
    "\n",
    "    i, j = torch.meshgrid(\n",
    "        torch.arange(W, dtype=torch.float32),\n",
    "        torch.arange(H, dtype=torch.float32),\n",
    "        indexing=\"xy\",\n",
    "    )\n",
    "    ds = torch.stack(\n",
    "        [(i - W * 0.5) / focal, -(j - H * 0.5) / focal, -torch.ones_like(i)], dim=-1\n",
    "    )\n",
    "    rays_d = ds @ c2w[:3, :3].T\n",
    "    rays_o = torch.broadcast_to(c2w[:3, -1], rays_d.shape)\n",
    "\n",
    "    return rays_d, rays_o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/yenchenlin/nerf-pytorch/issues/41\n",
    "H, W = train_dataset.shape\n",
    "focal = train_dataset.focal\n",
    "c2w = train_dataset.transform_matrixes[5]\n",
    "\n",
    "\n",
    "rays_d, rays_o = get_rays(H, W, focal, c2w)\n",
    "rays_d.shape, rays_o.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.rand(3, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strat_sampling(t_near, t_far, n):\n",
    "    samples = (torch.arange(n) + torch.rand(n)) * (t_far - t_near) / n\n",
    "    return samples\n",
    "\n",
    "samples = strat_sampling(0, 10, 5)\n",
    "\n",
    "gap = torch.diff(samples)\n",
    "samples, gap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(p: torch.Tensor, L: int) -> torch.Tensor:\n",
    "    assert len(p.shape) == 2\n",
    "    D = p.size(1)\n",
    "\n",
    "    # Z denotes transformed input p\n",
    "    # Z_ij becomes 2^i * p_i * p_j for each i in 0..L-1 and each component j in 1..3\n",
    "    # Thus dimension is <B, D, L>\n",
    "    z = (2 ** torch.arange(L).repeat(D, 1)) * (torch.pi *  p[..., None])\n",
    "\n",
    "    # X denotes the encoded value for each transformed input\n",
    "    x1 = torch.sin(z)\n",
    "    x2 = torch.cos(z)\n",
    "    \n",
    "    # We want ordering sin(x) cos(x) sin(y) cos(y) sin(z) cos(z) repeated for each element in 1..L\n",
    "    # First we stack encoding into a matrix, then we flatten the matrix to put each row side by side.\n",
    "    x = torch.stack((x1, x2), dim=3)        # <B, D, L, 2>\n",
    "    x = x.swapaxes(1, 2)                    # <B, L, D, 2>\n",
    "    x = x.reshape(-1, 2 * D * L)        # Finally, flatten to shape <B, 2*D*L>\n",
    "\n",
    "    return x\n",
    "\n",
    "B = 4\n",
    "L1 = 10\n",
    "L2 = 4\n",
    "\n",
    "tx = torch.arange(B * 3).reshape(B, 3)\n",
    "td = torch.arange(B * 3).reshape(B, 3)\n",
    "tex = positional_encoding(tx, L1)\n",
    "ted = positional_encoding(tx, L2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
