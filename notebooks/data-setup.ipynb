{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib as pl\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import io\n",
    "from torchvision import transforms\n",
    "import json\n",
    "\n",
    "data_source_name = \"NeRF_Data\"\n",
    "dataset_name = \"nerf_synthetic\"\n",
    "scene_name = \"lego\"\n",
    "\n",
    "root_data_dir = pl.Path('./data/')\n",
    "data_path = root_data_dir / data_source_name / dataset_name / scene_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "digit_pattern = re.compile(r\"\\d+\")\n",
    "\n",
    "\n",
    "def load_transforms(tfs_path: pl.Path) -> tuple[float, list[float], list[torch.FloatTensor]]:\n",
    "    with open(tfs_path, \"r\") as f:\n",
    "        transforms = json.load(f)\n",
    "\n",
    "    cam_angle_x = float(transforms[\"camera_angle_x\"])\n",
    "    rotations = []\n",
    "    transform_matrixes = []\n",
    "\n",
    "    for frame in transforms[\"frames\"]:       \n",
    "        # Assume ordered\n",
    "        rotation = float(frame[\"rotation\"])\n",
    "        transform_matrix = torch.FloatTensor(frame[\"transform_matrix\"])\n",
    "\n",
    "        rotations.append(rotation)\n",
    "        transform_matrixes.append(transform_matrix)\n",
    "\n",
    "    return cam_angle_x, rotations, transform_matrixes\n",
    "        \n",
    "\n",
    "def extract_digit_from_path_name(path: pl.Path) -> int:\n",
    "    match = digit_pattern.search(path.name)\n",
    "\n",
    "    if not match:\n",
    "        return None\n",
    "\n",
    "    return int(match.group(0))\n",
    "\n",
    "\n",
    "def load_img_paths(imgs_path: pl.Path):\n",
    "    paths = imgs_path.iterdir() # Ordered lexagraphically\n",
    "    paths = sorted(paths, key=extract_digit_from_path_name) # Ordered numerically\n",
    "    return paths\n",
    "\n",
    "\n",
    "def load_frame(\n",
    "        imgs_path: pl.Path,\n",
    "        rotations: list[float],\n",
    "        tf_matrixes: list[torch.FloatTensor],\n",
    "        idx: int\n",
    "    ) -> tuple[torch.Tensor, float, torch.Tensor]:\n",
    "    img_path = imgs_path[idx]\n",
    "    rotation = rotations[idx]\n",
    "    tf_matrix = tf_matrixes[idx]\n",
    "    img = io.read_image(str(img_path), mode=io.ImageReadMode.RGB_ALPHA)\n",
    "\n",
    "    return img, rotation, tf_matrix\n",
    "\n",
    "\n",
    "class FrameDataset(Dataset):\n",
    "    def __init__(\n",
    "            self,\n",
    "            data_path: pl.Path,\n",
    "            data_mode: str,  # 'train', 'val', 'test'\n",
    "            ex_idx: int = 5\n",
    "        ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.imgs_path = data_path / data_mode\n",
    "        self.tfs_path = data_path / f\"transforms_{data_mode}.json\"\n",
    "\n",
    "        self.cam_angle_x, self.rotations, self.transform_matrixes = load_transforms(self.tfs_path)\n",
    "        self.img_paths = load_img_paths(self.imgs_path)\n",
    "\n",
    "        self.ex_img, *_ = self[ex_idx]\n",
    "        \n",
    "        self.C, self.H, self.W = self.ex_img.shape\n",
    "\n",
    "        self.focal = 0.5 * self.W / np.tan(0.5 * self.cam_angle_x)\n",
    "\n",
    "\n",
    "    @property\n",
    "    def shape(self) -> tuple[float, float]:\n",
    "        return self.H, self.W\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.transform_matrixes)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> tuple[torch.Tensor, float, torch.Tensor]:\n",
    "        img, rot, c2w = load_frame(self.img_paths, self.rotations, self.transform_matrixes, idx)\n",
    "        C, H, W = img.shape\n",
    "        #img = transforms.Resize((H//8, W//8), antialias=True)(img)\n",
    "        return img, rot, c2w\n",
    "\n",
    "\n",
    "train_dataset = FrameDataset(data_path, \"train\")\n",
    "\n",
    "print(train_dataset.ex_img.shape)\n",
    "print(train_dataset.ex_img[0].max(), train_dataset.ex_img[3].max())\n",
    "plt.imshow(train_dataset.ex_img.T.swapaxes(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_dataset.img_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rays(\n",
    "    H: int, W: int, focal: float, c2w: torch.Tensor\n",
    ") -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    # Ported from https://github.com/bmild/nerf/blob/master/run_nerf_helpers.py#L123\n",
    "\n",
    "    i, j = torch.meshgrid(\n",
    "        torch.arange(W, dtype=torch.float32),\n",
    "        torch.arange(H, dtype=torch.float32),\n",
    "        indexing=\"xy\",\n",
    "    )\n",
    "    ds = torch.stack(\n",
    "        [(i - W * 0.5) / focal, -(j - H * 0.5) / focal, -torch.ones_like(i)], dim=-1\n",
    "    )\n",
    "    rays_d = ds @ c2w[:3, :3].T\n",
    "    rays_o = torch.broadcast_to(c2w[:3, -1], rays_d.shape)\n",
    "\n",
    "    return rays_d, rays_o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/yenchenlin/nerf-pytorch/issues/41\n",
    "H, W = train_dataset.shape\n",
    "focal = train_dataset.focal\n",
    "c2w = train_dataset.transform_matrixes[5]\n",
    "\n",
    "\n",
    "rays_d, rays_o = get_rays(H, W, focal, c2w)\n",
    "rays_d.shape, rays_o.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#H, W = 8, 8\n",
    "L1 = 10\n",
    "L2 = 4\n",
    "D = 3\n",
    "\n",
    "batch_size = 1\n",
    "n_bins = 5\n",
    "n_rays = H * W\n",
    "n_samples = n_bins * n_rays\n",
    "\n",
    "t_near = 0.1\n",
    "t_far = 5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(p: torch.Tensor, L: int) -> torch.Tensor:\n",
    "    assert len(p.shape) == 4\n",
    "    B, NR, NB, D = p.shape\n",
    "    #p = p.reshape(B, NR*NB, D)\n",
    "\n",
    "    # Z denotes transformed input p\n",
    "    # Z_ij becomes 2^i * p_i * p_j for each i in 0..L-1 and each component j in 1..3\n",
    "    # Thus dimension is <B, D, L>\n",
    "    z = (2 ** torch.arange(L).repeat(D, 1)) * (torch.pi *  p[..., None])\n",
    "\n",
    "    # X denotes the encoded value for each transformed input\n",
    "    x1 = torch.sin(z)\n",
    "    x2 = torch.cos(z)\n",
    "    \n",
    "    # We want ordering sin(x) cos(x) sin(y) cos(y) sin(z) cos(z) repeated for each element in 1..L\n",
    "    # First we stack encoding into a matrix, then we flatten the matrix to put each row side by side.\n",
    "    x = torch.stack((x1, x2), dim=5)        # <B, NR, NB, D, L, 2>\n",
    "    x = x.swapaxes(3, 4)                    # <B, NR, NB, L, D, 2>\n",
    "    x = x.reshape(B, NR*NB, 2 * D * L)        # Finally, flatten to shape <B, N, 2*D*L>\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "\n",
    "tmp_o = torch.randn(batch_size * n_samples * D).reshape(batch_size, n_rays, n_bins, D)\n",
    "tmp_d = torch.randn(batch_size * n_samples * D).reshape(batch_size, n_rays, n_bins, D)\n",
    "tex = positional_encoding(tmp_o, L1)\n",
    "ted = positional_encoding(tmp_o, L2)\n",
    "\n",
    "tex.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestNet(nn.Module):\n",
    "    def __init__(self, L1, L2, n_components, n_hidden):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d1 = n_components * 2 * L1\n",
    "        self.d2 = n_components * 2 * L2\n",
    "\n",
    "        self.lin1 = nn.Linear(self.d1, n_hidden + 1)\n",
    "        self.lin2 = nn.Linear(n_hidden + self.d2, 3)\n",
    "\n",
    "    def forward(self, o: torch.Tensor, d: torch.Tensor) -> torch.Tensor:\n",
    "        # o: <B, NS, D*2*L> \n",
    "        # d: <B, NS, D*2*L> \n",
    "        assert len(o.shape) == 3\n",
    "        assert len(d.shape) == 3\n",
    "        assert o.size(2) == self.d1\n",
    "        assert d.size(2) == self.d2\n",
    "\n",
    "        z1 = self.lin1(o)\n",
    "\n",
    "        x = torch.cat((d, z1[..., 1:]), dim=2)\n",
    "\n",
    "        sigma = z1[..., 0]  # <B, NS>\n",
    "        rgb = self.lin2(x)  # <B, NS, 3>\n",
    "\n",
    "        sigma = nn.functional.relu(sigma)\n",
    "        rgb = nn.functional.sigmoid(rgb)\n",
    "\n",
    "        return rgb, sigma\n",
    "\n",
    "tn = TestNet(L1, L2, 3, 128)\n",
    "tmp_c, tmp_sigma = tn(tex, ted)\n",
    "tmp_c.shape, tmp_sigma.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strat_sampling(N: int, t_near: float, t_far: float) -> torch.Tensor:\n",
    "    samples = (torch.arange(N) + torch.rand(N)) * (t_far - t_near) / N  # <N>\n",
    "    return samples\n",
    "\n",
    "\n",
    "def get_t(batch_size, n_rays, n_bins, t_near, t_far) -> torch.Tensor:\n",
    "    t = strat_sampling(batch_size * n_rays * n_bins, t_near, t_far).reshape(batch_size, n_rays, n_bins)\n",
    "    dt = torch.diff(t, dim=-1)\n",
    "    return t, dt\n",
    "\n",
    "tmp_t, tmp_dt = get_t(batch_size, n_rays, n_bins, t_near, t_far)\n",
    "tmp_t.shape, tmp_dt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expected_color(c, sigma, dt):\n",
    "    # c: <B, N, 3>\n",
    "    # sigma: <B, NS>\n",
    "    # delta: <B, NR, NB>\n",
    "    # B: batch size\n",
    "    # N: number of samples\n",
    "    # C: number of components \n",
    "\n",
    "    assert len(sigma.shape) == 2\n",
    "    assert len(dt.shape) == 3\n",
    "    assert len(c.shape) == 3\n",
    "\n",
    "    B, NR, NB = dt.shape\n",
    "    NB = NB + 1\n",
    "    C = c.size(-1)\n",
    "\n",
    "    # Unpack from n_samples to n_rays x n_bins\n",
    "    sigma = sigma.reshape(B, NR, NB)\n",
    "    c = c.reshape(B, NR, NB, C)\n",
    "    \n",
    "    mul = dt * sigma[..., :-1]\n",
    "\n",
    "    # Compute cumuluative probability, \n",
    "    # Since equation (3) sums T_i from i=1 to i-1, we set the first value to (exp 0 = 1) and ignore the last value\n",
    "    T = torch.exp(-torch.cumsum(mul, dim=-1))\n",
    "    T = torch.cat((torch.ones(B, NR, 1), T), dim=-1)[..., :-1]\n",
    "\n",
    "    # Since we do no have a delta for the last value, \n",
    "    # we directly set the last value of w to T at i=N,\n",
    "    # which is the dot product between sigma and delta \n",
    "    T_N = torch.einsum(\"brn,brn->br\", dt, sigma[..., :-1])[..., None]\n",
    "    w = T * (1 - torch.exp(-mul) )\n",
    "    w = torch.cat((w, T_N), dim=-1)\n",
    "\n",
    "    c_hat = torch.einsum(\"brn,brnc->brc\", w, c)\n",
    "    return c_hat\n",
    "\n",
    "tmp_c_hat = expected_color(tmp_c, tmp_sigma, tmp_dt).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RayDataset(Dataset):\n",
    "    def __init__(self, frame_dataset: FrameDataset) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.frame_dataset = frame_dataset \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.frame_dataset)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        C_r, _, c2w = self.frame_dataset[idx]\n",
    "        C_r = C_r / 255.0\n",
    "\n",
    "        focal = self.frame_dataset.focal\n",
    "        H, W = self.frame_dataset.shape\n",
    "        \n",
    "        r_o, r_d = get_rays(H, W, focal, c2w)\n",
    "        return r_o, r_d, C_r\n",
    "        \n",
    "        \n",
    "train_render_dataset = RayDataset(train_dataset)\n",
    "train_render_loader = DataLoader(train_render_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_o, r_d, C_r = next(iter(train_render_dataset))\n",
    "r_o = r_o[None, ...]\n",
    "r_d = r_d[None, ...]\n",
    "C_r = C_r[None, ...]\n",
    "\n",
    "B = r_o.size(0)\n",
    "t, dt = get_t(B, n_rays, n_bins, t_near, t_far)\n",
    "\n",
    "r_d = nn.functional.normalize(r_d, dim=-1)\n",
    "\n",
    "r_o = r_o.reshape(B, -1, 1, 3)\n",
    "r_d = r_d.reshape(B, -1, 1, 3)\n",
    "t = t[..., None]\n",
    "\n",
    "pts = r_o + t * r_d\n",
    "pts, r_d\n",
    "print(pts.shape, r_d.shape)\n",
    "\n",
    "ex = positional_encoding(pts, L1)\n",
    "ed = positional_encoding(r_d, L2)\n",
    "\n",
    "print(ex.shape, ed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as ptl\n",
    "\n",
    "class LitNerf(ptl.LightningModule):\n",
    "    def __init__(self, scene_model: nn.Module, learning_rate: float = 3e-4):\n",
    "        super().__init__()\n",
    "        self.scene_model = scene_model\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        r_o, r_d, C_r = batch\n",
    "\n",
    "        B = r_o.size(0)\n",
    "        t, dt = get_t(B, n_rays, n_bins, t_near, t_far)\n",
    "\n",
    "        r_d = nn.functional.normalize(r_d, dim=-1)\n",
    "\n",
    "        r_o = r_o.reshape(B, -1, 1, 3)\n",
    "        r_d = r_d.reshape(B, -1, 1, 3)\n",
    "        r_d = r_d.repeat(1, 1, n_bins, 1)\n",
    "        t = t[..., None]\n",
    "\n",
    "        C_r = C_r[:, :3].reshape(B, 3, -1).swapaxes(1, 2)\n",
    "\n",
    "        x = r_o + t * r_d\n",
    "        ex = positional_encoding(x, L1)\n",
    "        ed = positional_encoding(r_d, L2)\n",
    "\n",
    "        c, sigma = self.scene_model(ex, ed)\n",
    "        c_hat = expected_color(c, sigma, dt)\n",
    "\n",
    "        loss = self.criterion(c_hat, C_r)\n",
    "\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "\n",
    "\n",
    "nerf = LitNerf(tn)\n",
    "for i in range(2):\n",
    "    %prun nerf.training_step(next(iter(train_render_loader)), 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = ptl.Trainer()\n",
    "trainer.fit(nerf, train_dataloaders=train_render_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
