{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib as pl\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import io\n",
    "import json\n",
    "\n",
    "data_source_name = \"NeRF_Data\"\n",
    "dataset_name = \"nerf_synthetic\"\n",
    "scene_name = \"lego\"\n",
    "\n",
    "root_data_dir = pl.Path('./data/')\n",
    "data_path = root_data_dir / data_source_name / dataset_name / scene_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "digit_pattern = re.compile(r\"\\d+\")\n",
    "\n",
    "\n",
    "def load_transforms(tfs_path: pl.Path) -> tuple[float, list[float], list[torch.FloatTensor]]:\n",
    "    with open(tfs_path, \"r\") as f:\n",
    "        transforms = json.load(f)\n",
    "\n",
    "    cam_angle_x = float(transforms[\"camera_angle_x\"])\n",
    "    rotations = []\n",
    "    transform_matrixes = []\n",
    "\n",
    "    for frame in transforms[\"frames\"]:       \n",
    "        # Assume ordered\n",
    "        rotation = float(frame[\"rotation\"])\n",
    "        transform_matrix = torch.FloatTensor(frame[\"transform_matrix\"])\n",
    "\n",
    "        rotations.append(rotation)\n",
    "        transform_matrixes.append(transform_matrix)\n",
    "\n",
    "    return cam_angle_x, rotations, transform_matrixes\n",
    "        \n",
    "\n",
    "def extract_digit_from_path_name(path: pl.Path) -> int:\n",
    "    match = digit_pattern.search(path.name)\n",
    "\n",
    "    if not match:\n",
    "        return None\n",
    "\n",
    "    return int(match.group(0))\n",
    "\n",
    "\n",
    "def load_img_paths(imgs_path: pl.Path):\n",
    "    paths = imgs_path.iterdir() # Ordered lexagraphically\n",
    "    paths = sorted(paths, key=extract_digit_from_path_name) # Ordered numerically\n",
    "    return paths\n",
    "\n",
    "\n",
    "def load_frame(\n",
    "        imgs_path: pl.Path,\n",
    "        rotations: list[float],\n",
    "        tf_matrixes: list[torch.FloatTensor],\n",
    "        idx: int\n",
    "    ) -> torch.Tensor:\n",
    "    img_path = imgs_path[idx]\n",
    "    rotation = rotations[idx]\n",
    "    tf_matrix = tf_matrixes[idx]\n",
    "    img = io.read_image(str(img_path), mode=io.ImageReadMode.RGB_ALPHA)\n",
    "\n",
    "    return img, rotation, tf_matrix\n",
    "\n",
    "\n",
    "class FrameDataset(Dataset):\n",
    "    def __init__(\n",
    "            self,\n",
    "            data_path: pl.Path,\n",
    "            data_mode: str,  # 'train', 'val', 'test'\n",
    "            ex_idx: int = 5\n",
    "        ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.imgs_path = data_path / data_mode\n",
    "        self.tfs_path = data_path / f\"transforms_{data_mode}.json\"\n",
    "\n",
    "        self.cam_angle_x, self.rotations, self.transform_matrixes = load_transforms(self.tfs_path)\n",
    "        self.img_paths = load_img_paths(self.imgs_path)\n",
    "\n",
    "        self.ex_img, *_ = self[ex_idx]\n",
    "        self.C, self.H, self.W = self.ex_img.shape\n",
    "\n",
    "        self.focal = 0.5 * self.W / np.tan(0.5 * self.cam_angle_x)\n",
    "\n",
    "\n",
    "    @property\n",
    "    def shape(self) -> tuple[float, float]:\n",
    "        return self.H, self.W\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tfs_path)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        return load_frame(self.img_paths, self.rotations, self.transform_matrixes, idx)\n",
    "\n",
    "\n",
    "train_dataset = FrameDataset(data_path, \"train\")\n",
    "\n",
    "print(train_dataset.ex_img.shape)\n",
    "print(train_dataset.ex_img[0].max(), train_dataset.ex_img[3].max())\n",
    "plt.imshow(train_dataset.ex_img.T.swapaxes(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_dataset.img_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rays(\n",
    "    H: int, W: int, focal: float, c2w: torch.Tensor\n",
    ") -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    # Ported from https://github.com/bmild/nerf/blob/master/run_nerf_helpers.py#L123\n",
    "\n",
    "    i, j = torch.meshgrid(\n",
    "        torch.arange(W, dtype=torch.float32),\n",
    "        torch.arange(H, dtype=torch.float32),\n",
    "        indexing=\"xy\",\n",
    "    )\n",
    "    ds = torch.stack(\n",
    "        [(i - W * 0.5) / focal, -(j - H * 0.5) / focal, -torch.ones_like(i)], dim=-1\n",
    "    )\n",
    "    rays_d = ds @ c2w[:3, :3].T\n",
    "    rays_o = torch.broadcast_to(c2w[:3, -1], rays_d.shape)\n",
    "\n",
    "    return rays_d, rays_o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/yenchenlin/nerf-pytorch/issues/41\n",
    "H, W = train_dataset.shape\n",
    "focal = train_dataset.focal\n",
    "c2w = train_dataset.transform_matrixes[5]\n",
    "\n",
    "\n",
    "rays_d, rays_o = get_rays(H, W, focal, c2w)\n",
    "rays_d.shape, rays_o.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H, W = 8, 8\n",
    "B = 7\n",
    "L1 = 10\n",
    "L2 = 4\n",
    "D = 3\n",
    "\n",
    "n_bins = 5\n",
    "n_rays = H * W\n",
    "n_samples = n_bins * n_rays\n",
    "\n",
    "t_near = 0.1\n",
    "t_far = 5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(p: torch.Tensor, L: int) -> torch.Tensor:\n",
    "    assert len(p.shape) == 3\n",
    "    B, N, D = p.shape\n",
    "\n",
    "    # Z denotes transformed input p\n",
    "    # Z_ij becomes 2^i * p_i * p_j for each i in 0..L-1 and each component j in 1..3\n",
    "    # Thus dimension is <B, D, L>\n",
    "    z = (2 ** torch.arange(L).repeat(D, 1)) * (torch.pi *  p[..., None])\n",
    "\n",
    "    # X denotes the encoded value for each transformed input\n",
    "    x1 = torch.sin(z)\n",
    "    x2 = torch.cos(z)\n",
    "    \n",
    "    # We want ordering sin(x) cos(x) sin(y) cos(y) sin(z) cos(z) repeated for each element in 1..L\n",
    "    # First we stack encoding into a matrix, then we flatten the matrix to put each row side by side.\n",
    "    x = torch.stack((x1, x2), dim=4)        # <B, N, D, L, 2>\n",
    "    x = x.swapaxes(2, 3)                    # <B, N, L, D, 2>\n",
    "    x = x.reshape(B, N, 2 * D * L)        # Finally, flatten to shape <B, N, 2*D*L>\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "\n",
    "tx = torch.randn(B * n_samples * D).reshape(B, n_samples, D)\n",
    "td = torch.randn(B * n_samples * D).reshape(B, n_samples, D)\n",
    "tex = positional_encoding(tx, L1)\n",
    "ted = positional_encoding(tx, L2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestNet(nn.Module):\n",
    "    def __init__(self, L1, L2, n_components, n_hidden):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d1 = n_components * 2 * L1\n",
    "        self.d2 = n_components * 2 * L2\n",
    "\n",
    "        self.lin1 = nn.Linear(self.d1, n_hidden + 1)\n",
    "        self.lin2 = nn.Linear(n_hidden + self.d2, 3)\n",
    "\n",
    "    def forward(self, o: torch.Tensor, d: torch.Tensor) -> torch.Tensor:\n",
    "        # o: <B, NS, D*2*L> \n",
    "        # d: <B, NS, D*2*L> \n",
    "        assert len(o.shape) == 3\n",
    "        assert len(d.shape) == 3\n",
    "        assert o.size(2) == self.d1\n",
    "        assert d.size(2) == self.d2\n",
    "\n",
    "        z1 = self.lin1(o)\n",
    "\n",
    "        x = torch.cat((d, z1[..., 1:]), dim=2)\n",
    "\n",
    "        sigma = z1[..., 0]  # <B, NS>\n",
    "        rgb = self.lin2(x)  # <B, NS, 3>\n",
    "\n",
    "        return rgb, sigma\n",
    "\n",
    "tn = TestNet(L1, L2, 3, 128)\n",
    "tc, tsigma = tn(tex, ted)\n",
    "tc.shape, tsigma.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strat_sampling(N: int, t_near: float, t_far: float) -> torch.Tensor:\n",
    "    samples = (torch.arange(N) + torch.rand(N)) * (t_far - t_near) / N  # <N>\n",
    "    return samples\n",
    "\n",
    "\n",
    "def get_delta(samples: torch.Tensor, max_delta: float = 1E10) -> torch.Tensor:\n",
    "    # samples: <B, NR, NB>\n",
    "    B, NR, NB = samples.shape\n",
    "    delta = torch.diff(samples, append=torch.ones(B, NR, 1) * max_delta, dim=-1)  # <B, NR, NB>\n",
    "    return delta\n",
    "\n",
    "\n",
    "\n",
    "samples = strat_sampling(B * n_samples, t_near, t_far).reshape(B, n_rays, n_bins)\n",
    "delta = get_delta(samples)\n",
    "\n",
    "samples.shape, delta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expected_color(c, sigma, delta):\n",
    "    # c: <B, N, 3>\n",
    "    # sigma: <B, NS>\n",
    "    # delta: <B, NR, NB>\n",
    "    # B: batch size\n",
    "    # N: number of samples\n",
    "    # C: number of components \n",
    "\n",
    "    assert len(sigma.shape) == 2\n",
    "    assert len(delta.shape) == 3\n",
    "    assert len(c.shape) == 3\n",
    "\n",
    "    B = delta.size(0)\n",
    "\n",
    "    delta = delta.reshape(B, -1)\n",
    "    mul = delta * sigma\n",
    "\n",
    "    T = torch.exp(-torch.cumsum(mul, dim=1))\n",
    "    T = torch.cat((torch.ones(B, 1), T), dim=1)[..., :-1]\n",
    "\n",
    "    w = T * (1 - torch.exp(-mul))\n",
    "\n",
    "    c_hat = torch.einsum(\"bn,bnc->bc\", w, c)\n",
    "    return c_hat\n",
    "\n",
    "expected_color(tc, tsigma, delta)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
